
# Url处理

•100GB url 文件，使用 1GB 内存计算出出现次数 top100 的 url 和出现的次数

## 要求

1. 输入数据自己构造

2. 实现思路，测试，瓶颈分析，优化思路，优化前后结果对比，最好都能写出来

提示：

· 注意代码可读性，添加必要的注释（英文）

· 注意代码风格与规范，添加必要的单元测试和文档

· 注意异常处理，尝试优化性能

## 实现思路

主体思路：将Url先做hash，然后映射到小文件中。在每个小文件中使用堆找出Top100，再进行合并。

数据分析：将小文件的个数设置为500个，假设对于不同的Url来说，Hash值的分布是均匀的，则在每个小文件中，不同的Url的量级为0.2G，是内存可以存下的范围，可以对其进行次数统计。

算法流程：该算法主要分为4个部分

1. 读取数据，使用Hash函数对每个URL进行Hash，然后将Hash结果对500取模，将该Url映射到对应的小文件中。

2. 对于每个小文件来说，读取数据，使用哈希表来统计每个Url出现的次数。遍历一遍哈希表，将结果放入一个小根堆中，并维护堆的大小在100以内。

3. 将500个小根堆依次合并，并维护堆的大小在100以内。将堆中的数据输出。

4. 清除程序所创建的临时文件。

## 瓶颈分析和优化思路

1. 硬盘IO效率不高。在尝试在此方向进行优化，还未有结果。

## 使用方式

假设url文件保存在data.txt文件中，其中格式为每行一个url。

运行如下命令，结果会输出到名为answer.txt文件中。

``` bash
make
./main
```

## 测试

### 10GB test

```bash
make makedata
./makedata 10
./main
```
耗时18min左右
